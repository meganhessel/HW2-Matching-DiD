---
title: "ðŸŒ¬ï¸ðŸ—³ Assignment 2: Wind Turbines, Matching, and Difference-in-Differences"
subtitle: "Replicate causal inference identification strategies in Stokes (2015) "
author: "EDS 241 / ESM 244 (DUE: 2/4/26)"
format:
  html:
    theme: sketchy
    css: styles.css
date: "January 26, 2026"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

### Assignment instructions

Working with classmates to troubleshoot code and concepts is encouraged. If you collaborate, list collaborators at the top of your submission.

All written responses must be written independently (in your own words).

Keep your work readable: Use clear headings and label plot elements thoughtfully.

Assignment submission (YOUR NAME): \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

------------------------------------------------------------------------

### Introduction

In this assignment I will be doing political weather forecasting except the â€œstormsâ€ I care about are electoral swings that might follow local wind turbine development.

In Stokes (2015), the idea is that a policy with diffuse benefits (cleaner electricity) can create concentrated local costs (turbines nearby), and those local opponents may â€œsend a signalâ€ at the ballot box (i.e., NIMBYISM). Your job is to use two statistical tools:

-   Matching: Can we create a more apples-to-apples comparison between precincts that did vs. did not end up near turbine proposals?
-   Fixed effects + Difference-in-Differences: Can we use repeated elections to estimate how within-precinct changes in turbine exposure relate to changes in incumbent vote share?

------------------------------------------------------------------------

### This repo is replicating the matching and fixed effects analyses from study:

> Stokes (2015): *"Electoral Backlash against Climate Policy: A Natural Experiment on Retrospective Voting and Local Resistance to Public Policy*.

-   **Study:** [Stokes (2015) - Article](https://drive.google.com/file/d/1y2Okzjq2EA43AW5JzCvFS8ecLpeP6NKh/view?usp=sharing)
-   **Data source:** [Dataverse-Stokes2015](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SDUGCC)

::: callout
`NOTE:` Replication of study estimates will be approximate. An alternative matching procedure and fixed effects estimation package are utilized in this assignment for illustration purposes.
:::

------------------------------------------------------------------------

### Setup: Load libraries

0.  Load libraries (+ install if needed)

```{r}

library(tidyverse)
library(here)
library(janitor)
library(jtools)

library(gtsummary)
library(gt)

library(MatchIt) # matching
library(cobalt)  # balance + love plots

library(fixest) # fast fixed effects
library(scales) # plotting

```

------------------------------------------------------------------------

### Part 1: Study Background

#### **1A.** Dive into the details of the study design and evaluation plan

> Goal: Get familiar with the study setting, environmental issue, and policy under evaluation.

::: callout
`NOTE:` Read over study to inform your response to the assignment questions. For this assignment we will skip-over sections that describe the *Instrumental Variables* identification strategy. We will cover instrumental variable designs weeks 6-7.
:::

**1A.Q1** Summarize the environmental policy issue, the outcome of interest, and the intervention being evaluated. Be sure to include a brief description of each of the following key elements of the study: unit of analysis, outcome, treatment, comparison group):

-   The centrist Liberal Party created a "feed-in tariffâ€ (FIT) policy in the early 2000s to phase-out coal by subsidizing new, low-carbon technologies (ie wind). In 2009, a law passed that removed local veto power in renewable energy placement. The "not in my backyard" (NIMBY) thought process may have caused local voting to push the Liberal Party out of the government in the 2011 election. This, this study investigates if proximity to wind energy (intervention/treatment) impacted voting behaviors (outcome) with fixed effects estimators and instrumental variable estimators. To do this, Stokes studied the electoral precincts (average \~350 voters each) within the 26 districts.

    -   Treatment: precincts that had a proposed or operational wind turbine within its boundaries

    -   Control: precincts without wind turbines

    -   Outcome: Liberal Party vote in provincial elections (2003, 2007, 2011)

**1A.Q2** Why might turbine proposals be correlated with baseline political preferences or rural areas? Provide 2 plausible mechanisms, and explain why that creates confounding.

-   Wind turbines need open, unobstructive land to be effective. Therefore, wind turbine ideal placement is in rural areas. However, the baseline political preferences in rural communities tends to be more conservative, who would vote against the Liberal Party pre and post the FIT policy (the intervention). Studying the *change* in voting behavior in these communities is faulty and confounded since the people dealing with the treatment (wind turbines) have a continual pre-existing preferences against the treatment.

-   Before 2009 (when the law to remove local veto power), poorer communities most likely disproportional faced the burden of renewable energy construction. Wealthier, more urban communities have a greater ability to resist and/or block turbine proposals due to more money, power, time, and easier communication. Thus, wind turbine placement could be disproportionally placed in poorer communities which would already a pre-existing baseline political preferences.

-   These confounding variables impact the outcome, creating a fundamental problem in the study as treatment assignment is not randomized.

------------------------------------------------------------------------

#### **1B.** Break down the causal inference strategy and identify threats to identification:

**1B.Q1** What is the key identifying assumption for a fixed effects / Difference-in-Difference design? Explain how this assumption when satisfied provides evidence of causal effect:

-   Assumption: Parallel trends! Prior to the intervention, the treatment and control group must have similar trends in the voting behavior.

-   If the 2 groups are similar in there voting behaviors pre-intervention, it is save to assume the treatment group's counter factual would follow the control group trends. Parallel trends create a realistic counter factual we can use to understand intervention effects.

**1B.Q2** What is the reason for using a fixed effects approach from a causal inference perspective? Summarize within the context of study (in your own words).

-   From a causal inference perspective, fix effects remove omitted variable bias from time-invariant factors and remove between group variation. The fixed effect approach controls for any pre-exiting political or rural baseline beliefs, studing the change in voting shares within the precincts after intervention.

**1B.Q3** What part of the SUTVA assumption is most likely violated in the context of this study design (and why)?

-   SUTVA has 2 main assumptions: no interference and consistency. No interference means each treatment effect in one precincts would not effect the other. However, political voting behavior typically has active engagement within and between communities. Stokes discuss the protests and campaign events against the Liberal Party. Therefore, one community beliefs can easily effect and empower other precincts outcomes. SUTVA's consistency assumption means each treatment must be administered consistently to all treatment groups. Yet, treatment groups can have variable proposed or operational wind turbines due to differences in space. Hence, both SUTVA assumptions are violated.

**1B.Q4** Why does spillover matter when estimating an unbiased treatment effect?

-   In a perfect world, control and treatment groups would be un-affected by each other. However, this rarely is possible. Because of communication and interactions between communities, precincts actions can impact each other. This interaction and influence cause bias in treatment effect because it creates new confounding variables that effect outcomes in both groups.

**1B.Q5** How do the authors assess the risk of spillovers, and what analytic choice do they make to attempt to mitigate the risk that spillover biases the causal estimate?

-   In this study, spillover is due to the enormous wind turbines that can be seen in neighboring communities. To reduce the spillover treatment bias, Stoke studied voting behaviors in relation to distance from the turbines, creating 6km buffers that excludes these communities from the control group.

------------------------------------------------------------------------

### Part 2: Matching

------------------------------------------------------------------------

We will start by evaluating the 2007 survey (cross-sectional) data. Treatment is defined by whether a precinct is near a turbine proposal (within 3 km).

> Goal: Match precincts using pre-treatment covariates and then estimate the effect of proposed wind turbines on incumbent vote share.

#### **2A.** Load data for matching

1.  Read in data file `stokes15_survey2007.csv`
2.  Code `precinct_id` and `district_id` as factors
3.  Take a look at the data

```{r}
# Load in data 
match_data <- read_csv(here::here("data", "stokes15_survey2007.csv"))

# Change cols to factors 
match_data$precinct_id <- as.factor(match_data$precinct_id)
match_data$district_id <- as.factor(match_data$district_id)

# Look at the data 
head(match_data)
```

**2A.Q1** Intuition check: **Why match?** Explain rationale for using this method.

-   Due to confounding variables, there is pre-existing differences in the groups. Matching chooses communities that are similar, so treatment and control groups are homogeneous. This reduces the bias between the treatment and control groups.

------------------------------------------------------------------------

#### **2B.** Check imbalance (before matching)

-   Create a covariate *balance table* comparing treated and control precincts
-   Treatment indicator: `proposed_turbine_3km`
-   Include pre-treatment covariates: `log_home_val_07`, `p_uni_degree`, `log_median_inc`, `log_pop_denc`
-   Use the `tbl_summary()` function from the `{gtsummary}` package.

```{r}

match_data %>% 
    select( # select column we want to compare 
        log_home_val_07, p_uni_degree, log_median_inc, log_pop_denc, proposed_turbine_3km
    ) %>% 
    tbl_summary ( # Create balance table 
        by = proposed_turbine_3km, 
        statistic = list(
            all_continuous() ~ "{mean} ({sd})",
            all_categorical() ~ "{n} ({p}%)"
    )) %>%  # modify headers 
  modify_header(label ~ "**Covariate**") %>%
  modify_spanning_header(c("stat_1", "stat_2") ~ "**Group**")
 
```

**2B.Q1** Summarize the table output: Which covariates look balanced/imbalanced?

-   All covariates look pretty balanced. The covariate with the largest difference between control and treatment is the `log_pop_denc`. But `log_pop_denc` also has a large standard deviation, so there is not much of an imbalance.

**2B.Q2** Describe in your own words why these covariates might be expected to confound the treatment estimate:

-   These covariates relate to economic status and home location (rural vs urban) which are all confounding variables (discussed in **1A.Q2**) possibly impacting the communities voting behavior.

------------------------------------------------------------------------

**2B.Q3** Intuition check: What type of data do you need to conduct a matching analysis?

-   Panel data is necessary when conducting matching analysis. Panel data is when observations are repeated in various clusters/groups/levels.

------------------------------------------------------------------------

### Conduct matching estimation using the {`MatchIt`} package:

ðŸ“œ [Documentation - MatchIt](https://kosukeimai.github.io/MatchIt/)

Learning goals:

-   Approximate the Mahalanobis matching method used in Stokes (2015)
-   Implement another common matching approach called `propensity score matching`

::: callout
`NOTE`: In the replication code associated with Stokes (2015) the {`AER`} package is used for Mahalanobis matching. In this assignment we use the {`MatchIt`} package. The results are comparable but will not be exactly the same.
:::

------------------------------------------------------------------------

### 2C. Mahalanobis nearest-neighbor matching

-   Conduct Mahalanobis matching\
-   Use nearest-neighbor match without replacement using Mahalanobis distance
-   Use 1-to-1 matching (match one control unit to each treatment unit)
-   Extract the matched data using `match.data()`

```{r}
set.seed(2412026)

match_model <- matchit(
    # Treatment_indicator ~  Pre_treatment_covariates
    proposed_turbine_3km ~ log_home_val_07 + p_uni_degree + log_median_inc + log_pop_denc,
  data = match_data, 
  method = "nearest",       # Nearest neighbor matching
  distance = "mahalanobis", # Mahalanobis distance
  ratio = 1,                # Match one control unit to one treatment unit (1:1 matching)
  replace = FALSE           # Control observations are not replaced
)

# Extract matched data
matched_data <- match.data(match_model)

```

```{r}
summary(match_model)
```

**2C.Q1** Using the `summary()` output: Which covariate had the largest and smallest `Std. Mean Diff.` before matching. Next, compare largest/smallest `Std. Mean Diff.` after matching.

**COMPARE TO OTHERS. IDK IF LOOK AT [ABOSOLUTE VALUES]{.underline} (lowest = closest to 0) OR NOT (lowest = most negative)**

-   Before matching, `log_pop_den` covariate had the largest `Std. Mean Diff` and `log_median_inc` covariate had the smallest.

-   After matching, `log_pop_den` covariate had the largest `Std. Mean Diff` and `log_median_inc` covariate had the smallest.

------------------------------------------------------------------------

#### 2D. Create a "love plot" using `love.plot()` â¤ï¸

ðŸ“œ [Documentation - cobalt](https://ngreifer.github.io/cobalt/)

-   Plot mean differences for data before & after matching across all pre-treatment covariates
-   This is an effective way to evaluate how effective matching was at achieving balance.

------------------------------------------------------------------------

-   Make a love plot of standardized mean differences (SMDs) before vs after matching.
-   Include a threshold line at 0.1.
-   In love plot display `mean.diffs`

```{r}

new_names <- data.frame(
    old = c("log_home_val_07", "p_uni_degree", "log_median_inc", "log_pop_denc"),
    new = c("Home Value (log)", "Pe <- rcent University Degree",
            "Median Income (log)", "Population Density (log)"))

# Love plot
love.plot(match_model, stats = "mean.diffs",
          thresholds = c(m = 0.1),
          var.names = new_names)

```

**2D.Q1** Interpret the love plot in your own words:

-   Love plots visualize standardized mean differences (before and after matching). This love plot shows population density and percent university degree was imbalanced between groups before matching. After matching, all confounding variables are similar between groups, reducing bias.

------------------------------------------------------------------------

### Propensity score matching

------------------------------------------------------------------------

#### 2E. Propensity Score Matching (PSM)

-   Estimate 1:1 nearest-neighbor Propensity Score Matching
-   Same code as above except change `distance = "logit"`

```{r}

set.seed(2412026)

propensity_scores <- matchit(
    # Treatment_indicator ~  Pre_treatment_covariates
    proposed_turbine_3km ~ log_home_val_07 + p_uni_degree + log_median_inc + log_pop_denc,
    data = match_data, 
    method = "nearest", # Nearest neighbor matching
    distance = "logit", # Propensity Score Matching (PSM)
    ratio = 1, # Match one control unit to one treatment unit (1:1 matching)
    replace = FALSE # Control observations are not replaced
)
    
```

------------------------------------------------------------------------

#### Create table displaying covariate balance using `cobalt::bal.tab()`

ðŸ“œ [Documentation - cobalt](https://ngreifer.github.io/cobalt/)

Use `bal.tab()` to report balance before and after matching.

```{r}
# reporting balance from propensity_scores
bal.tab(propensity_scores, 
        var.names = new_names)

# Looking at propensity_scores love plot 
love.plot(propensity_scores, stats = "mean.diffs",
          thresholds = c(m = 0.1),
          var.names = new_names)

```

**2E.Q1** Compare Mahalanobis vs propensity score matching. Which method did a better job at achieving balance?

-   When comparing Mahalanobis vs propensity score `Std. Mean Diff.`, Mahalanobis did a better job of achieving balance.

------------------------------------------------------------------------

#### 2F. Estimate an effect in the matched sample

Using the matched data (Mahalanobis method), estimate the effect of treatment on the change in incumbent vote share (`change_liberal`).

```{r}
# Regression: vote shares based on turbine proximity 
reg_match <- lm(
    change_liberal ~ proposed_turbine_3km, 
    data = matched_data
)

summ(reg_match, model.fit = FALSE)
```

Coefficient Interpretations

-   Without treatment, Liberal Party voting share is expected to average about -0.07. With treatment (proximity to wind turbines), there is an expected 0.06 decrease in Liberal Party voting shares. Standard deviation is small, and there is statistical significance in this prediction.

**2F.Q1** Have you identified a causal estimate using this approach: Why or why not?

-   No. Matching only balances the *observed* covariates (home value, education, income, population density). While this improves balance and variable independence, matching does not address *unobserved* confounders that impact turbine placement and voting changes.

**2F.Q2** When using a matching method, what is the main threat to causal identification?

-   Matching method assume there is no other systematic differences between treated and control groups other than the chosen matched variables. Unobserved variables are unaccounted for and can create bias in the outcomes, threatening causal identification reliability.

**2F.Q3** Describe why the treatment estimate represents the `Average Treatment for the Treated (ATT)` and explain why this is the case relative to estimation of the `Average Treatment Effect (ATE)`.

-   In this study, we matched by finding one similar control unit for each treated unit. Therefore, while all treated units are included, only some of the control units were included in the `match_model`. ATT focuses on the average effect of the treated based on those who received treatment. Because the model includes all treatment groups, the estimatations represent the ATT. On the other hand, ATE measures the average effect of a treatment across the entire population. Because the model does not include the entire population, `match_model` does not estimate the ATE.

------------------------------------------------------------------------

### Part 3: Panel Data, Fixed Effects, and Difference-in-Difference

**Data source:** [Dataverse-Stokes2015](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SDUGCC)

------------------------------------------------------------------------

#### **3A:** Read in the panel data + code variables `precinct_id` and `year` as factors

```{r}
# Read in data 
panel_data <- read_csv(here::here("data", "Stokes15_panel_data.csv"))

# HINT: Try running `tabyl(panel_data$year)`. Review article to make sense of the row numbers (n).
tabyl(panel_data$year)

# Year and precinct_id as factor 
panel_data$year <- as.factor(panel_data$year)
panel_data$precinct_id <- as.factor(panel_data$precinct_id)

```

**3A.Q1:** Why are there 18,558 rows in `panel_data`?

-   There are a total of 6186 precincts that have repeating measurementss (voting shares) for 3 different years (2003, 2007, 2011).

-   6186 \* 3 = 18,558

```{r}
# How many years are included in the panel? - 3 years: 2003, 2007, 2011
unique(panel_data$year)
# How many precincts are there? - 6186 
str(panel_data)
```

**3A.Q2:** How many unique precincts are *ever treated* (i.e., `proposed` & `operational`)?

-   Of the treatment precincts, 184 had proposed turbines and 52 had operational turbines.

```{r}

panel_data %>%
  group_by(precinct_id) %>% # Grouping for each precincts
  summarise( # seperating prosposed vs operational turbines 
    ever_proposed    = any(proposed_turbine == 1, na.rm = TRUE),
    ever_operational = any(operational_turbine == 1, na.rm = TRUE),
    .groups = "drop") %>%
  summarise( # Total count 
    n_ever_proposed    = sum(ever_proposed),
    n_ever_operational = sum(ever_operational))

```

------------------------------------------------------------------------

#### **3B.** Plot and evaluate parallel trends: Replicate `Figure.2` (Stokes, 2015)

1.  Create indicators for whether each precinct is ever treated by 2011 (`treat_p`, `treat_o`; separate indicator for proposals and operational turbines).
2.  Plot mean incumbent vote share by year for treated vs control precincts (with 95% CIs).
3.  Facet by turbine type (proposed & operational)

Step 1: Prepare data

```{r}
# Indicators: control vs treatment group (treatment: `treat_o` vs `treat_p`)
trends_data <- panel_data %>%
  group_by(precinct_id) %>% # group by precinct 
  mutate( 
    treat_p = as.integer(any(proposed_turbine == 1, na.rm = TRUE)),  # ever proposed (in any year)
    treat_o = as.integer(any(operational_turbine == 1, na.rm = TRUE))) %>% # ever operational (in any year)
  ungroup() %>% 
  pivot_longer(c(treat_p, treat_o), # new col `turbine_type` - with treat_p and treat_o
               names_to = "turbine_type", values_to = "treat") %>% 
  mutate( # Make `turbine_type` a factor 
      turbine_type = factor(turbine_type,
                            levels = c("treat_p", "treat_o"),
                            labels = c("Proposed turbines", "Operational turbines")),  
    status = if_else(treat == 1, "Treated", "Control"), # control and treatment col 
    year   = factor(year)) # Making sure year is a factor 

```

Step 2: Create trends plot

```{r}

pd <- position_dodge(width = 0.15)

trends_data %>%
  group_by(turbine_type, status, year) %>% # Grouping Group (T vs C), year, and Treatment type (operational vs proposed)
  summarise( 
    mean = mean(perc_lib, na.rm = TRUE), # mean of voting share
    n    = sum(!is.na(perc_lib)), # count 
    se   = sd(perc_lib, na.rm = TRUE) / sqrt(n), # Standard error 
    ci   = qt(.975, df = pmax(n - 1, 1)) * se, # Confidence intervals 
    .groups = "drop") %>%
ggplot(aes(year, mean, color = status, group = status)) + # PLOT! 
  geom_line(position = pd, linewidth = 1.2) + # line plot with points 
  geom_point(position = pd, size = 2.6) +
  geom_errorbar( # CI 
    aes(ymin = mean - ci, ymax = mean + ci),
    position = pd, width = .12, linewidth = .7, color = "black") +
  facet_wrap(~ turbine_type, nrow = 1) + # 2 plots - opertaional vs proposed turbines 
  scale_color_manual(values = c(Control = "#0072B2", Treated = "#B22222")) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  coord_cartesian(ylim = c(.20, .57)) +
  labs(
    title = "Figure 2. Trends in the Governing Partyâ€™s Vote Share",
    x = "Election Year",
    y = "Liberal Party Vote Share",
    color = NULL) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = "bottom",
    strip.text = element_text(face = "bold"))

```

**3B.Q1:** Write a short paragraph assessing the parallel trends assumption for each outcome.

-   Background: In this study, pre-treatment is considered the 2003 voting shares. In 2006, the FIT policy was initated on a small scale. Therefore, early turbines proposals and operations began in 2007. In 2009, the law that refused communities to reject renewable energy started. Full effects of the 2009 Green Energy Act were seen in 2011.

-   The graph above shows parallel trends between treatment and control groups in 2003 and 2007 (the baseline and early stages of the intervention). Treatment effect is exemplified in the treatment groups change of Liberal Party voting shares in 2011. Thus, the parallel trend assumption is met, but should be considered with caution due to the limited pre-treatment measurements. One pre-treatment measurement and one early-intervention stage measurement, its not enough observations to be super confident on meeting the parallel trends assumption.

------------------------------------------------------------------------

### Estimating Fixed Effects Models (DiD) for proposals

$$
\text{Y}_{it}
=  \alpha_0 +
\beta \cdot (\text{proposed_turbine}_{it})
+ \gamma_i
+ \delta_t
+ \varepsilon_{it}
$$

-   $Y_{it}$ is the vote share for the Liberal Party in precinct *i* in time *t*
-   $\beta$ is the treatment effect of a turbine being proposed within a precinct
-   $\gamma_i$ is the precinct fixed effect
-   $\delta_t$ is the year fixed effect

------------------------------------------------------------------------

### Example 1: Randomly sample 40 precincts

-   To illustrate the "dummy variable method" of estimating fixed effects using the the general `lm()` function we are going to randomly sample 40 precincts (20 "treated" precincts with proposed turbines).
-   If we attempted to use this approach with the full sample estimating all 6185 (n-1) precinct-level coefficients is impractical (it would take a long time).

```{r}
set.seed(40002026)

# Dataframe of just precinct_id and if have proposed_turbine
precinct_frame <- panel_data %>%
  group_by(precinct_id) %>% 
  summarise( # where proposed_turbine = 1 => TRUE --> convert results to an integers (0,1)
    proposed_turbine_any = as.integer(any(proposed_turbine == 1, na.rm = TRUE)),
    .groups = "drop"
  )

# Randomly selecting 20 precinct_ids 
ids_40 <- precinct_frame %>%
  group_by(proposed_turbine_any) %>%
  slice_sample(n = 20) %>%
  ungroup() %>%
  select(precinct_id)

# Get the panel_data for the randomly selected rows 
sample_40_precincts <- panel_data %>%
  semi_join(ids_40, by = "precinct_id") # semi_join: return all rows from x with a match in y.

```

------------------------------------------------------------------------

#### **3C:** Estimate a fixed effects model using `lm()` with fixed effects added for `precinct` and `year` using the sample of 40 precincts just created.

```{r}
# lm model with precinct and year as fixed 
model1_ff <- lm(
    perc_lib ~ proposed_turbine + precinct_id + year,
    data = sample_40_precincts 
)

summ(model1_ff, model.fit = FALSE, digits = 3)
```

```{r}
summ(model1_ff, model.fit = FALSE, digits = 3, robust = TRUE)
```

**3C.Q1:** Intuition check: Is the *signal-to-noise* ratio for the treatment estimate greater than *2-to-1*?

-   For the treatment estimate, the *signal-to-noise* ratio is less than the 2-to-1 threshold. The treatment effect is marginally significant (p = 0.067) and the estimate is twice as large as its standard error. Meaning there is signal but it is noisy with moderate uncertainty.

-   \|estimate\| / standard error = \|-0.057\| / 0.031 = 1.83871

> HINT: Add the argument `digits = 3` to the `summ()` function above

**3C.Q2:** Re-run the `summ()` function using the *heteroscedasticiy robust standard error adjustment* (`robust = TRUE`). Did the standard error (S.E.) estimates change? Explain why.

-   The standard error increases (0.031 â€“\> 0.039) and the p-value increases (0.067 â€“\> 0.147). With the *heteroscedasticiy robust standard error adjustment,* the treatment effect has greater variance and is non-significant.

-   The OLS standard error assumes homoscedasticity, where the variance of errors is constant across all observations. When relaxing that assumption with `robust = TRUE`, the variance can differ across observations. For instance, Liberal vote share can now vary between treated vs. control precincts, different precincts over time, and different election years.

-   The OLS did not account for heteroscedasticity which led to overconfident inference.

**3C.Q3:** Compare results of the model above to the findings from the fixed effects analysis in the Stokes (2015) study. Why might the results be similar or different?

-   Both the model above and Stokes (2015) study show a decline in Liberal voting shares in relation to proposed turbine implementation in precincts. However, the model above has larger uncertainty and no statistical significance. The models main difference is the sample size. Stokes model includes 18,558 observations (6,186 precincts for 3 years), whereas my model only includes 120 observations (40 precincts for 3 years ). While the model above is consistent with the patterns in Stokes model, the small sample decreases the model's statistical power and increases uncertainty.

**3C.Q4:** In your own words, explain why it is advantageous from a causal inference perspective to include year and precinct fixed effects. Explain how between-level and within-level variance is relevant to the problem of omitted variable bias (OVB).

-   Adding precinct as a fixed effect controls for all the possible time-invariant confounding differences between precincts, such as demographic, geographic, and pre-existing beliefs. Adding year as a fix effect controls for differences/trends that happened between 2003, 2007, and 2011 that effected all precincts equally. Adding fixed effects that help control between-level variance (differences across precincts) and within-level variance (changes within the same precinct over time) allow for unobserved and observed differences in the parameters to be "differenced out", reducing OVB.

------------------------------------------------------------------------

#### **3D.** Now using the full sample, estimate the treatment effect of wind turbine proposals on incumbent vote share. Use `feols()` from the `{fixest}` package to estimate the fixed effects.

See vignette here: [fixest walkthrough](https://cran.r-project.org/web/packages/fixest/vignettes/fixest_walkthrough.html#11_Estimation)

```{r}

model2_ff <-  feols(
    perc_lib ~ proposed_turbine | year + precinct_id, # Add fixed effects 
    data = panel_data, 
    cluster = ~precinct_id # cluster by precinct_id 
)
    

summary(model2_ff, model.fit = FALSE)
```

**3D.Q1:** Interpret the model results and translate findings to be clear to an audience that may not have a background in causal inference (Econometrics) methods.

In panel data settings, why is clustering by precinct important (i.e., `cluster = ~precinct_id`) ?â€

-   In this study, we are investigating if communities near wind turbines turned against the Liberal government that approved them. To understand the *change* of voting behavior, we compared precincts to themselves over time. We found that precincts with proposed wind turbines had a 4.157% decrease of Liberal Party vote shares, which shows local voter backlash.

-   A single precinct's voting trends over the years are correlated. For instances, how a community votes in 2003 is correlated to their votes in 2007. Clustering by precinct accounts for this within-precinct correlation.

------------------------------------------------------------------------

#### **3E.** Estimate the treatment effect of *operational wind turbines* on incumbent vote share. Use the same approach as the previous model.

```{r}

model3_ff <-  feols(
    perc_lib ~ operational_turbine | year + precinct_id, # Add fixed effects 
    data = panel_data, 
    cluster = ~precinct_id # cluster by precinct_id 
)
    
summary(model3_ff, model.fit = FALSE)

```

**3E.Q1:** Interpret the `model3_ff` results as clearly and **concisely** as you can.

-   Precincts with operational wind turbines had approximately a 9.276% decrease of Liberal Party vote shares compared to what would have happened without the turbines, which shows local voter backlash.

**3E.Q2:** Why do you think the effect of proposed wind turbines is different from operational wind turbines. Develop your own theory about why incumbent vote share is affected in this way. Use the Stokes (2015) study to inform your response as needed.

-   Physically having to deal with wind turbines in your communities is drastically different than possibly/planning to have turbines in your community. Communities that actually deal with wind turbines everyday have a larger grudge with Liberal Party, which leads to a larger reduction in Liberal Party voting shares.

------------------------------------------------------------------------

```{r, message=TRUE, echo=FALSE, eval=FALSE}

library(praise); library(cowsay)

praise("${EXCLAMATION}! ðŸš€ Great work - You are ${adjective}! ðŸ’«")

say("The End", "duck")
```
